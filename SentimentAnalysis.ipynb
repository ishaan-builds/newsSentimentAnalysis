{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd07812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d",
   "display_name": "Python 3.9.2 64-bit ('3.9')"
  },
  "metadata": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "test = input(\"What would you like to know about today?  \")\n",
    "stonks = ['AMD','AAPL','INTC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           publishedAt                                              title\n",
       "32 2021-04-23 12:00:25  The Rumor of a Potential Partnership Between L...\n",
       "24 2021-04-23 17:46:14  3 reasons to be bearish on Intel despite a new...\n",
       "46 2021-04-26 10:00:00  When Do Third-Party Cookies End? Apple (AAPL) ...\n",
       "52 2021-04-26 12:36:56  Apple (AAPL) Boosts U.S. Investment 20% Over F...\n",
       "17 2021-04-26 16:42:07  Tim Cook reportedly told Mark Zuckerberg that ..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>publishedAt</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>32</th>\n      <td>2021-04-23 12:00:25</td>\n      <td>The Rumor of a Potential Partnership Between L...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2021-04-23 17:46:14</td>\n      <td>3 reasons to be bearish on Intel despite a new...</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>2021-04-26 10:00:00</td>\n      <td>When Do Third-Party Cookies End? Apple (AAPL) ...</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>2021-04-26 12:36:56</td>\n      <td>Apple (AAPL) Boosts U.S. Investment 20% Over F...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2021-04-26 16:42:07</td>\n      <td>Tim Cook reportedly told Mark Zuckerberg that ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "url = 'https://newsapi.org/v2/everything?'\n",
    "\n",
    "# Specify the query and number of returns\n",
    "parameters = {\n",
    "    'qInTitle': test, # query phrase\n",
    "    'sortBy': 'popularity', # articles from popular sources and publishers come first\n",
    "    'pageSize': 100,  # maximum is 100 for developer version\n",
    "    'apiKey': '32dc1d81f85d44cd959b4428b0308bdd', # your own API key\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, params=parameters)\n",
    "\n",
    "# Convert the response to JSON format and store it in dataframe\n",
    "data = pd.DataFrame(response.json())\n",
    "news_df = pd.concat([data['articles'].apply(pd.Series)], axis=1)\n",
    "\n",
    "# Select data\n",
    "final_news = news_df.loc[:,['publishedAt','title']]\n",
    "\n",
    "# Filter to within one week\n",
    "final_news['publishedAt'] = pd.to_datetime(final_news['publishedAt'])\n",
    "final_news['publishedAt'] = final_news['publishedAt'].apply(lambda x: x.replace(tzinfo=None)) #removes timezone\n",
    "final_news = final_news[pd.to_datetime('now')-final_news['publishedAt']<=pd.to_timedelta(30, unit='d')]\n",
    "final_news.sort_values(by='publishedAt',inplace=True)\n",
    "final_news.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      label                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>According to Gran , the company has no plans t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>Technopolis plans to develop in stages an area...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>The international electronic industry company ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>With the new production plant the company woul...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>According to the company 's updated strategy f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "data = pd.read_csv(\"all-data.csv\",delimiter=',',encoding='latin-1',header=None)\n",
    "data.columns = [\"label\", \"text\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.text.values\n",
    "labels = data.label.values\n",
    "processed_features = [] \n",
    "def process2(): \n",
    "    # alternatively could use list version\n",
    "    #just different implementations u can consider, rly up to u\n",
    "    features = data.text.values\n",
    "    for text in features:\n",
    "        processed_features.append(process1(text))\n",
    "def process1(features):\n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features))\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "    # Converting to Lowercase\n",
    "    return processed_feature.lower()\n",
    "data.text = data.text.apply(process1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for installing nltk stopwords\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Use TF-IDF to vectorize our words\n",
    "#TF = freq of word in doc / total words in doc\n",
    "#IDF = log(total # of docs/# of docs containing word)\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "#max_features specifies 2500 most frequent words\n",
    "#max_df specifies words that occur in a max of 80% of docs\n",
    "#min_df specifies words that occur in at least 7 docs\n",
    "#stopwords are excluded, e.g. \"it\" or \"am\"\n",
    "processed_features = vectorizer.fit_transform(data.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 45  59  24]\n [  6 540  29]\n [  8 128 131]]\n              precision    recall  f1-score   support\n\n    negative       0.76      0.35      0.48       128\n     neutral       0.74      0.94      0.83       575\n    positive       0.71      0.49      0.58       267\n\n    accuracy                           0.74       970\n   macro avg       0.74      0.59      0.63       970\nweighted avg       0.74      0.74      0.72       970\n\n0.7381443298969073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead, we can create new processed_features by concatenating the two datasets\n",
    "#together and vectorizing\n",
    "processed_features = vectorizer.fit_transform(pd.concat([data.text, final_news.title])).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.23456712, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "#now this stuff becomes our modelling dataset\n",
    "processed_features[:data.shape[0]]\n",
    "#and this is our predicting dataset\n",
    "processed_features[data.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(processed_features[:data.shape[0]], labels, test_size=0.2, random_state=0)\n",
    "X_train = processed_features[:data.shape[0]]\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive',\n",
       "       'neutral', 'positive', 'neutral', 'positive', 'neutral',\n",
       "       'positive', 'neutral', 'neutral', 'positive', 'neutral', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'neutral', 'positive',\n",
       "       'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "predictions = text_classifier.predict(processed_features[data.shape[0]:])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}